{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWBPxj0edSrFEHtmaR6D22",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tushar0852/Project-perceptron/blob/main/project_perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxat2SACM5wg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuronUnit:\n",
        "    def __init__(self, num_inputs):\n",
        "        self.weights = np.random.rand(num_inputs)\n",
        "        self.bias = np.random.rand()\n",
        "        self.lrate = 0.1\n",
        "        self.value = 0.0\n",
        "        self.delta = 0.0"
      ],
      "metadata": {
        "id": "A_1OkEw1bzdz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralLayer:\n",
        "    def __init__(self, num_neurons, num_inputs):\n",
        "        self.neurons = [NeuronUnit(num_inputs) for _ in range(num_neurons)]\n",
        "        self.length = num_neurons\n"
      ],
      "metadata": {
        "id": "cbP_GKFBb5hV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiLayerNetwork:\n",
        "    def __init__(self, l_rate, transfer_function, transfer_function_derivative):\n",
        "        self.l_rate = l_rate\n",
        "        self.transfer_function = transfer_function\n",
        "        self.transfer_function_derivative = transfer_function_derivative\n",
        "        self.neural_layers = []\n",
        "\n",
        "    def prepare_layer(self, num_neurons, num_inputs):\n",
        "        layer = NeuralLayer(num_neurons, num_inputs)\n",
        "        self.neural_layers.append(layer)\n",
        "\n",
        "    def prepare_mlp_net(self, layers, l_rate, transfer_function, transfer_function_derivative):\n",
        "        self.l_rate = l_rate\n",
        "        self.transfer_function = transfer_function\n",
        "        self.transfer_function_derivative = transfer_function_derivative\n",
        "\n",
        "        self.neural_layers = []\n",
        "        for i in range(len(layers)):\n",
        "            num_neurons = layers[i]\n",
        "            num_inputs = layers[i - 1] if i != 0 else 0\n",
        "            self.prepare_layer(num_neurons, num_inputs)\n",
        "\n",
        "        print(\"Complete Multilayer Perceptron init.\")"
      ],
      "metadata": {
        "id": "n-Cxtz9Kb9mv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def execute(mlp, pattern):\n",
        "    # Set input values\n",
        "    for i in range(len(pattern.features)):\n",
        "        mlp.neural_layers[0].neurons[i].value = pattern.features[i]\n",
        "\n",
        "    # Forward pass\n",
        "    for k in range(1, len(mlp.neural_layers)):\n",
        "        for i in range(mlp.neural_layers[k].length):\n",
        "            nv = sum(mlp.neural_layers[k].neurons[i].weights[j] * mlp.neural_layers[k - 1].neurons[j].value\n",
        "                     for j in range(mlp.neural_layers[k - 1].length))\n",
        "            nv += mlp.neural_layers[k].neurons[i].bias\n",
        "            mlp.neural_layers[k].neurons[i].value = mlp.transfer_function(nv)\n",
        "\n",
        "    # Get output values\n",
        "    return [neuron.value for neuron in mlp.neural_layers[-1].neurons]\n"
      ],
      "metadata": {
        "id": "ey6xx1SycH2f"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backpropagate(mlp, pattern, expected_output):\n",
        "    no = execute(mlp, pattern)\n",
        "    error = 0.0\n",
        "\n",
        "    # Output layer\n",
        "    for i in range(mlp.neural_layers[-1].length):\n",
        "        e = expected_output[i] - no[i]\n",
        "        mlp.neural_layers[-1].neurons[i].delta = e * mlp.transfer_function_derivative(no[i])\n",
        "\n",
        "    # Hidden layers\n",
        "    for k in range(len(mlp.neural_layers) - 2, -1, -1):\n",
        "        for i in range(mlp.neural_layers[k].length):\n",
        "            e = sum(mlp.neural_layers[k + 1].neurons[j].delta * mlp.neural_layers[k + 1].neurons[j].weights[i]\n",
        "                    for j in range(mlp.neural_layers[k + 1].length))\n",
        "            mlp.neural_layers[k].neurons[i].delta = e * mlp.transfer_function_derivative(mlp.neural_layers[k].neurons[i].value)\n",
        "\n",
        "        # Update weights and biases\n",
        "        for i in range(mlp.neural_layers[k + 1].length):\n",
        "            for j in range(mlp.neural_layers[k].length):\n",
        "                mlp.neural_layers[k + 1].neurons[i].weights[j] += mlp.l_rate * mlp.neural_layers[k + 1].neurons[i].delta * mlp.neural_layers[k].neurons[j].value\n",
        "            mlp.neural_layers[k + 1].neurons[i].bias += mlp.l_rate * mlp.neural_layers[k + 1].neurons[i].delta\n",
        "\n",
        "    # Calculate global error\n",
        "    for i in range(len(expected_output)):\n",
        "        error += abs(no[i] - expected_output[i])\n",
        "\n",
        "    return error / len(expected_output)\n"
      ],
      "metadata": {
        "id": "lVLdnLaccZdh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp_train(mlp, patterns, mapped, epochs):\n",
        "    output = [0.0] * len(mapped)\n",
        "    epoch = 0\n",
        "\n",
        "    while epoch <= epochs:\n",
        "        total_error = 0.0\n",
        "        for pattern in patterns:\n",
        "            for io in range(len(output)):\n",
        "                output[io] = 0.0\n",
        "            output[int(pattern.single_expectation)] = 1.0\n",
        "            total_error += backpropagate(mlp, pattern, output)\n",
        "\n",
        "        # Print the average error after each epoch\n",
        "        avg_error = total_error / len(patterns)\n",
        "        print(f\"Epoch {epoch} - Average Error: {avg_error:.4f}\")\n",
        "\n",
        "        epoch += 1\n"
      ],
      "metadata": {
        "id": "TEDk2-26cdWe"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_predictions(mlp, patterns):\n",
        "    for i, pattern in enumerate(patterns):\n",
        "        output = execute(mlp, pattern)\n",
        "        print(f\"Pattern {i} - Input: {pattern.features}, Predicted: {output}, Expected: {pattern.single_expectation}\")\n"
      ],
      "metadata": {
        "id": "ObwNcuz0fQaF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n"
      ],
      "metadata": {
        "id": "QB_-XKmrffqV"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}